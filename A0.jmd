---
title : Assignment 0
author : Hongbo Du(1003568089)
options:
  eval: true #Set this to true if you'd like to evaluate the code in this document
---

```julia
# We will use unit testing to make sure our solutions are what we expect
# This shows how to import the Test package, which provides convenient functions like @test
using Test
# Setting a Random Seed is good practice so our code is consistent between runs
using Random # Import Random Package
Random.seed!(414); #Set Random Seed
# ; suppresses output, makes the writeup slightly cleaner.
# ! is a julia convention to indicate the function mutates a global state.
```

# Probability

## Variance and Covariance
Let $X$ and $Y$ be two continuous, independent random variables.

1. [3pts] Starting from the definition of independence, show that the independence of $X$ and $Y$ implies that their covariance is $0$.

Answer:

\begin{equation*}
\begin{split}
    \text{Cov}(X, Y) &= \mathbb{E}[(X-\mathbb{E}(X))(Y-\mathbb{E}(Y))] \qquad (\text{by definition})\\
    &= \mathbb{E}[(XY) - Y\mathbb{E}(X) - X\mathbb{E}(Y) + \mathbb{E}(X)\mathbb{E}(Y)]\\
    &= \mathbb{E}(X)\mathbb{E}(Y) - \mathbb{E}(Y)\mathbb{E}(X) - \mathbb{E}(X)\mathbb{E}(Y) + \mathbb{E}(X)\mathbb{E}(Y) \qquad (\text{by independence})\\
    &= 0
\end{split}
\end{equation*}

2. [3pts] For a scalar constant $a$, show the following two properties starting from the definition of expectation:

$$
\begin{align}
\mathbb{E}(X+aY) &= \mathbb{E}(X) + a\mathbb{E}(Y)\\
\text{var}(X + aY) &= \text{var}(X) + a^2 \text{var}(Y)
\end{align}
$$

Answer:

\begin{equation*}
\begin{split}
\mathbb{E}(X+aY) &= \mathbb{E}(X) + \mathbb{E}(aY) \qquad (\text{by independence})\\
&= \mathbb{E}(X) + a\mathbb{E}(Y)
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
\text{var}(X + aY) &= \mathbb{E}[(X + aY)^2] - [\mathbb{E}(X + aY)]^2 \qquad (\text{by definition})\\
&= \mathbb{E}[X^2 + 2aXY + a^2Y^2] - [\mathbb{E}(X)]^2 - 2a\mathbb{E}(X)\mathbb{E}(Y) - a^2[\mathbb{E}(Y)]^2\\
&= \mathbb{E}(X^2) - [\mathbb{E}(X)]^2 + a^2\mathbb{E}(Y^2)  - a^2[\mathbb{E}(Y)]^2\\
&= \text{var}(X) + a^2 \text{var}(Y)
\end{split}
\end{equation*}


## 1D Gaussian Densities

1. [1pts] Can a probability density function (pdf) ever take values greater than $1$?

  Answer: Yes, as long as the input value is finite non-negative.

2.  Let $X$ be a univariate random variable distributed according to a Gaussian distribution with mean $\mu$ and variance $\sigma^2$.

* [[1pts]] Write the expression for the pdf:

  Answer:

  $$ f_X (x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} \exp\bigg\{-\frac{1}{2}\Big(\frac{x-\mu}{\sigma}\Big)^2\bigg\} $$

* [[2pts]] Write the code for the function that computes the pdf at $x$ with default values $\mu=0$ and $\sigma = \sqrt{0.01}$:

    Answer:

```julia
function gaussian_pdf(x; mean=0., variance=0.01)
  sd = sqrt(variance)
  pdf = (1)/(sd * sqrt(2 * pi)) * exp(-0.5 * ((x - mean)/(sd))^2)
  return pdf
end
```

Test your implementation against a standard implementation from a library:
```julia
# Test answers
using Distributions: pdf, Normal # Note Normal uses N(mean, stddev) for parameters
@testset "Implementation of Gaussian pdf" begin
  x = randn()
  @test gaussian_pdf(x) ≈ pdf.(Normal(0.,sqrt(0.01)),x)
  # ≈ is syntax sugar for isapprox, typed with `\approx <TAB>`
  # or use the full function, like below
  @test isapprox(gaussian_pdf(x,mean=10., variance=1) , pdf.(Normal(10., sqrt(1)),x))
end;
```


3. [1pts] What is the value of the pdf at $x=0$? What is probability that $x=0$ (hint: is this the same as the pdf? Briefly explain your answer.)

    Answer: The value of the pdf at $x=0$ is a finite non-negtive number, and the probability of $x=0$ is 0. These two are not the same.

4. A Gaussian with mean $\mu$ and variance $\sigma^2$ can be written as a simple transformation of the standard Gaussian with mean $0.$ and variance $1.$.

  * [[1pts]] Write the transformation that takes $x \sim \mathcal{N}(0.,1.)$ to $z \sim \mathcal{N}(\mu, \sigma^2)$:

    Answer: Consider the transformation $$x = \frac{z - \mu}{\sigma}$$ which takes $z$ to $x$. Thus, the transformation $$z = x\sigma + \mu$$ takes $x$ to $z$.

  * [[2pts]] Write a code implementation to produce $n$ independent samples from $\mathcal{N}(\mu, \sigma^2)$ by transforming $n$ samples from $\mathcal{N}(0.,1.)$.

Answer

```julia
function sample_gaussian(n; meam=0., variance=0.01)
  # n samples from standard gaussian
  x = rand(Normal(0, 1), n)

  # transform x to sample z from N(mean,variance)
  mu = transpose(zeros(true, length(x)))
  z = x * sqrt(variance) + mu
  return z
end;
```

[2pts] Test your implementation by computing statistics on the samples:

```julia
using Statistics: mean, var
@testset "Numerically testing Gaussian Sample Statistics" begin
  # Sample 100000 samples with your function and use mean and var to
  # compute statistics.
  # tests should compare statistics against the true mean and variance from arguments.
  sample = sample_gaussian(100000)
  s_mean = mean(sample) # true mean
  s_var = var(sample) # true variance
  @test isapprox(s_mean, 0, atol=1e-2)
  @test isapprox(s_var, 0.01, atol=1e-2)
end;
```


5. [3pts] Sample $10000$ samples from a Gaussian with mean $10.$ an variance $2$. Plot the **normalized** `histogram` of these samples. On the same axes `plot!` the pdf of this distribution.
Confirm that the histogram approximates the pdf.

```julia
using Plots

hist = histogram(rand(Normal(10, 2), 10000))
plot!(hist)
```


# Calculus

## Manual Differentiation

Let $x,y \in \mathbb{R}^m$, $A \in \mathbb{R}^{m \times n}$, and square matrix $B \in \mathbb{R}^{m \times m}$.
And where $x'$ is the transpose of $x$.
Answer the following questions in vector notation.

1. [1pts] What is the gradient of $x'y$ with respect to $x$?

Answer:

\begin{equation*}
    \begin{split}
        \nabla_{\bm{x}} \bm{x'y}
        = \frac{\partial}{\partial x}\bigg[\sum^{m}_{i=1} x_i y_i\bigg]
        = \bm{y}
     \end{split}
\end{equation*}


2. [1pts] What is the gradient of $x'x$ with respect to $x$?

Answer:

\begin{equation*}
    \begin{split}
        \nabla_{\bm{x}} \bm{x'x}
        = \frac{\partial}{\partial x} \bigg[\sum^{m}_{i=1} x_i x_i\bigg]
        = \frac{\partial}{\partial x} \bigg[\sum^{m}_{i=1} x^2_i\bigg]
        = 2\bm{x}
     \end{split}
\end{equation*}


3. [2pts] What is the Jacobian of $x'A$ with respect to $x$?

Answer:

\begin{equation*}
    \begin{split}
        \bm{J}_{\bm{x}} \bm{x'}A &= \frac{\partial}{\partial x}\bigg(\sum^{m}_{i=1} x_i a_{i1} \dots \sum^{m}_{i=1} x_i a_{in} \bigg)\\
        &= \begin{pmatrix}
        \frac{\partial}{\partial x_1} [\sum^{m}_{i=1} x_i a_{i1}] & \dots & \frac{\partial}{\partial x_m} [\sum^{m}_{i=1} x_i a_{i1}]\\
        \vdots & \ddots & \vdots \\
        \frac{\partial}{\partial x_1} [\sum^{m}_{i=1} x_i a_{in}] & \dots & \frac{\partial}{\partial x_m} [\sum^{m}_{i=1} x_i a_{in}]
        \end{pmatrix}\\
        &= \begin{pmatrix}
        a_{11} & \dots & a_{m1}\\
        \vdots & \ddots & \vdots \\
        a_{1n} & \dots & a_{mn}
        \end{pmatrix}\\
        &= A'
     \end{split}
\end{equation*}

4. [2pts] What is the gradient of $x'Bx$ with respect to $x$?

Answer:

\begin{equation*}
    \begin{split}
        \nabla_{\bm{x}} \bm{x'}B\bm{x} &= \frac{\partial}{\partial x_k} \bigg[x_1\sum^{m}_{i=1}b_{i1} x_i + \dots + x_m \sum^{m}_{i=1}b_{im} x_i\bigg]\\
        &= \frac{\partial}{\partial x_k} \bigg[x_1 b_{k1} x_k + \dots + x_m b_{km} x_k\bigg] + \frac{\partial}{\partial x_k} \bigg[x_k\sum^{m}_{i=1}b_{ik} x_i\bigg]\\
        &= \sum^{m}_{j=1} x_{j} b_{kj} + \sum^{m}_{i=1} b_{ik} x_{i}\\
        &= B\bm{x} + B'\bm{x}\\
        &= (B + B')\bm{x}
    \end{split}
\end{equation*}

## Automatic Differentiation (AD)

Use one of the accepted AD library (Zygote.jl (julia), JAX (python), PyTorch (python))
to implement and test your answers above.

### [1pts] Create Toy Data


```julia
# Choose dimensions of toy data
m = 3
n = 2

# Make random toy data with correct dimensions
x = rand(Int, m)
y = rand(Int, m)
A = rand(Int, m, n)
B = rand(Int, m, m)
```
[1pts] Test to confirm that the sizes of your data is what you expect:
```julia
# Make sure your toy data is the size you expect!
@testset "Sizes of Toy Data" begin
  #TODO: confirm sizes for toy data x,y,A,B
  #hint: use `size` function, which returns tuple of integers.
  @test isapprox(size(x, 1), m)
  @test isapprox(size(y, 1), m)
  @test isapprox(size(A, 1), m)
  @test isapprox(size(A, 2), n)
  @test isapprox(size(B, 1), m)
  @test isapprox(size(B, 1), m)
end;
```

### Automatic Differentiation

1. [1pts] Compute the gradient of $f_1(x) = x'y$ with respect to $x$?

```julia
# Use AD Tool
using Zygote: gradient
# note: `Zygote.gradient` returns a tuple of gradients, one for each argument.
# if you want just the first element you will need to index into the tuple with [1]

f1(x) = transpose(x) * y
df1dx = gradient(x -> f1(x), x)[1]
```

2. [1pts] Compute the gradient of $f_2(x) = x'x$ with respect to $x$?

```julia
f2(x) = transpose(x) * x
df2dx = gradient(x -> f2(x), x)[1]
```

3. [1pts] Compute the Jacobian of $f_3(x) = x'A$ with respect to $x$?

If you try the usual `gradient` fucntion to compute the whole Jacobian it would give an error.
You can use the following code to compute the Jacobian instead.

```julia
function jacobian(f, x)
    y = f(x)
    n = length(y)
    m = length(x)
    T = eltype(y)
    j = Array{T, 2}(undef, n, m)
    for i in 1:n
        j[i, :] .= gradient(x -> f(x)[i], x)[1]
    end
    return j
end
```

[2pts] Briefly, explain why `gradient` of $f_3$ is not well defined (hint: what is the dimensionality of the output?) and what the `jacobian` function is doing in terms of calls to `gradient`.
Specifically, how many calls of `gradient` is required to compute a whole `jacobian` for $f : \mathbb{R}^m \rightarrow \mathbb{R}^n$?

Answer:

The very important takeaway here is that, with AD, `gradient`s are cheap but full `jacobian`s are expensive.

```julia
f3(x) = transpose(x) * A
df3dx = jacobian(f3, x) #using jacobian
```



4. [1pts] Compute the gradient of $f_4(x) = x'Bx$ with respect to $x$?

```julia
f4(x) = transpose(x) * B * x
df4dx = gradient(x -> f4(x), x)[1]
```


5. [2pts] Test all your implementations against the manually derived derivatives in previous question
```julia
# Test to confirm that AD matches hand-derived gradients
@testset "AD matches hand-derived gradients" begin
  @test df1dx == y
  @test df2dx == 2 * x
  @test df3dx == transpose(A)
  @test df4dx == (B + transpose(B)) * x
end
```
